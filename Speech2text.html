<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Speech-to-Text</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom styles for the microphone pulse animation */
        @keyframes pulse-ring {
            0% {
                transform: scale(0.33);
            }
            80%, 100% {
                opacity: 0;
            }
        }
        @keyframes pulse-dot {
            0% {
                transform: scale(0.8);
            }
            50% {
                transform: scale(1);
            }
            100% {
                transform: scale(0.8);
            }
        }
        .mic-button-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 120px;
        }
        .mic-button-container.listening .pulse-ring {
            animation: pulse-ring 1.8s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
        }
        .mic-button-container.listening .pulse-dot {
            animation: pulse-dot 1.2s infinite;
        }
        #result-container {
            min-height: 200px;
            white-space: pre-wrap;
        }
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
</head>
<body class="bg-gray-50 flex items-center justify-center min-h-screen p-4 font-sans">
    <div class="w-full max-w-lg bg-white shadow-2xl rounded-xl p-6 md:p-8">
        <h1 class="text-3xl font-bold text-gray-800 mb-2 text-center">Voice Dictation Pad</h1>
        <p class="text-gray-500 mb-6 text-center">Continuous real-time speech transcription.</p>

        <!-- Status Message and Language Selector -->
        <div class="flex flex-col sm:flex-row justify-between items-center mb-6 space-y-4 sm:space-y-0">
            <p id="status-message" class="text-sm font-medium text-blue-600">Click the microphone to start.</p>
            <div class="flex items-center space-x-2">
                <label for="language-select" class="text-sm text-gray-600 hidden sm:block">Language:</label>
                <select id="language-select" class="p-2 border border-gray-300 rounded-lg text-sm focus:ring-blue-500 focus:border-blue-500">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish (Spain)</option>
                    <option value="fr-FR">French (France)</option>
                    <option value="de-DE">German (Germany)</option>
                    <option value="ja-JP">Japanese (Japan)</option>
                </select>
            </div>
        </div>

        <!-- Microphone Button Area -->
        <div id="mic-control" class="mic-button-container">
            <button id="microphone-button" class="relative flex items-center justify-center w-20 h-20 bg-red-500 rounded-full shadow-lg transition-all duration-300 hover:bg-red-600 focus:outline-none focus:ring-4 focus:ring-red-300 focus:ring-opacity-75">
                <!-- Pulsing Ring (for listening state) -->
                <span class="pulse-ring absolute inline-flex w-full h-full bg-red-400 opacity-75 rounded-full"></span>
                <!-- Inner Dot (for listening state) -->
                <span class="pulse-dot relative inline-flex rounded-full w-12 h-12 bg-white">
                    <!-- Microphone Icon (Lucide icon converted to SVG for single-file mandate) -->
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-6 h-6 text-red-500 m-auto">
                        <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"/><path d="M19 10v2a7 7 0 0 1-14 0v-2"/><line x1="12" x2="12" y1="19" y2="22"/>
                    </svg>
                </span>
            </button>
        </div>
        
        <!-- Transcription Results -->
        <div class="mt-8">
            <h3 class="text-xl font-semibold text-gray-700 mb-2">Transcription:</h3>
            <div id="result-container" class="p-4 border border-gray-200 rounded-lg bg-gray-100 transition-all duration-300">
                <!-- Final Transcript -->
                <span id="final-transcript" class="text-gray-800 text-base leading-relaxed"></span>
                <!-- Interim (Live) Transcript -->
                <span id="interim-transcript" class="text-gray-400 text-base leading-relaxed font-light italic"></span>
            </div>
            <button id="copy-button" class="mt-4 w-full py-2 px-4 bg-green-500 text-white font-medium rounded-lg shadow hover:bg-green-600 transition-colors duration-200 disabled:opacity-50" disabled>
                Copy Text
            </button>
        </div>
        
        <!-- Error/Support Message -->
        <p id="support-message" class="text-sm text-red-500 mt-6 text-center" style="display:none;">
            ðŸš¨ Web Speech API not supported by this browser. Please use Google Chrome or Microsoft Edge for the best experience.
        </p>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // Check for Web Speech API support and handle vendor prefixes
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            const micButton = document.getElementById('microphone-button');
            const micControl = document.getElementById('mic-control');
            const statusMessage = document.getElementById('status-message');
            const finalTranscriptSpan = document.getElementById('final-transcript');
            const interimTranscriptSpan = document.getElementById('interim-transcript');
            const copyButton = document.getElementById('copy-button');
            const langSelect = document.getElementById('language-select');
            const supportMessage = document.getElementById('support-message');

            let recognition = null;
            let isRecording = false;
            let finalTranscript = '';

            // Helper function to update UI state
            function setUIState(isListening) {
                isRecording = isListening;
                micControl.classList.toggle('listening', isListening);
                micButton.style.backgroundColor = isListening ? 'rgb(34 197 94)' : 'rgb(239 68 68)'; // Green when listening, Red when stopped
                micButton.classList.toggle('bg-green-500', isListening);
                micButton.classList.toggle('bg-red-500', !isListening);
                statusMessage.textContent = isListening ? 'Listening... Speak clearly now.' : 'Recording stopped. Click to restart.';
                if (!isListening) {
                    // Update final transcript display only when recording stops
                    finalTranscriptSpan.textContent = finalTranscript;
                    interimTranscriptSpan.textContent = '';
                }
                copyButton.disabled = finalTranscript.trim().length === 0;
            }

            // --- API Initialization and Event Handlers ---
            if (typeof SpeechRecognition === 'undefined') {
                micButton.disabled = true;
                statusMessage.style.display = 'none';
                supportMessage.style.display = 'block';
                return;
            }

            // Create a new recognition instance
            function createRecognitionInstance() {
                recognition = new SpeechRecognition();
                
                // Advanced Configurations
                recognition.continuous = true;       // Keep listening until stopped by the user
                recognition.interimResults = true;   // Provide preliminary results as the user speaks
                recognition.lang = langSelect.value; // Set the selected language

                // 1. When recognition starts
                recognition.onstart = () => {
                    setUIState(true);
                };

                // 2. When results are returned
                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    let newFinalSegment = '';

                    // Loop through the results to separate final and interim parts
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            // This is a confirmed, final piece of text
                            newFinalSegment += transcript + ' ';
                        } else {
                            // This is the real-time, in-progress prediction
                            interimTranscript += transcript;
                        }
                    }

                    // Append the new final segment to the master final transcript
                    if (newFinalSegment.length > 0) {
                        finalTranscript += newFinalSegment;
                    }

                    // Update the UI
                    finalTranscriptSpan.textContent = finalTranscript;
                    interimTranscriptSpan.textContent = interimTranscript;
                    copyButton.disabled = finalTranscript.trim().length === 0;
                };

                // 3. When the recognition service stops unexpectedly (e.g., due to inactivity)
                recognition.onend = () => {
                    if (isRecording) {
                        // If we are still in the recording state, restart automatically for continuous operation
                        statusMessage.textContent = 'Speech service disconnected. Restarting...';
                        try {
                             recognition.start();
                        } catch (e) {
                            console.error('Error restarting recognition:', e);
                            setUIState(false);
                            statusMessage.textContent = 'An error occurred during continuous listening. Please try again.';
                        }
                    } else {
                        statusMessage.textContent = 'Recording stopped.';
                    }
                };

                // 4. Error handling
                recognition.onerror = (event) => {
                    console.error('Speech Recognition Error:', event.error);
                    setUIState(false);
                    if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                        statusMessage.textContent = 'Microphone access denied. Please check your browser settings.';
                    } else {
                        statusMessage.textContent = `Error: ${event.error}. Click to retry.`;
                    }
                };
            }

            createRecognitionInstance();

            // --- Button and Event Listeners ---
            
            micButton.addEventListener('click', () => {
                if (isRecording) {
                    // Stop listening
                    recognition.stop();
                    setUIState(false);
                } else {
                    // Start listening (clear previous session first)
                    finalTranscript = '';
                    finalTranscriptSpan.textContent = '';
                    interimTranscriptSpan.textContent = '';
                    recognition.lang = langSelect.value; // Apply selected language before starting
                    recognition.start();
                }
            });

            langSelect.addEventListener('change', () => {
                if (isRecording) {
                    // Restart recognition with new language
                    recognition.stop();
                    createRecognitionInstance(); // Re-create instance to apply new language
                    recognition.start();
                } else {
                    // Pre-emptively update the recognition instance language
                    createRecognitionInstance();
                    statusMessage.textContent = `Language set to ${langSelect.options[langSelect.selectedIndex].text}. Click the microphone to start.`;
                }
            });

            copyButton.addEventListener('click', () => {
                const textToCopy = finalTranscriptSpan.textContent.trim();
                if (textToCopy) {
                    // Fallback method for copying in browser environments
                    const el = document.createElement('textarea');
                    el.value = textToCopy;
                    document.body.appendChild(el);
                    el.select();
                    document.execCommand('copy');
                    document.body.removeChild(el);
                    
                    const originalText = copyButton.textContent;
                    copyButton.textContent = 'Copied!';
                    setTimeout(() => {
                        copyButton.textContent = originalText;
                    }, 1500);
                }
            });
        });
    </script>
</body>
</html>